
```{r echo = FALSE}
set.seed(12042016)
```

# Correlation and Regression {#analysis2}

## Intended Learning Outcomes {#ilo-analysis2}

By the end of this chapter you should be able to:

* Perform and visualize Pearson and Spearman correlations
* Construct simple and multiple linear and logistic regression models

As with Comparing Means, this chapter is unlikely to cover everything you want to know about regression, particularly if you use advanced techniques for your own research. Instead, it will give you the basic coding skills you need to understand how such models are constructed from a programming perspective, and point you towards additional resources that have a heavier focus on statistical theory.

## Walkthrough video {#walkthrough-analysis2}

We encourage you to read the workbook and attempt each step on your own before watching the video as this will help consolidate your learning (it may feel harder but making mistakes is informative and will help you learn more in the long-run).

<iframe width="560" height="315" src="https://www.youtube.com/embed/QlxKKX31AMI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Set-up {#setup-analysis2}

* Create and save a new R Markdown document named `chapter_6.Rmd`, get rid of the default template text from line 11 onwards.
* Add the below code to the set-up chunk and then run the code to load the packages and data.You may need to install the packages if you don't have them installed already.

```{r echo = FALSE}
library(patchwork)
```



```{r setup-wrangle, message=FALSE}
library(tidyverse)   
library(broom)
library(psych)
library(easystats)
library(gapminder)
library(medicaldata)
data("polyps")
data("opt")
data("gapminder")
```  


## Correlation

As with ANOVA, there are a huge number of packages and functions you can use to perform correlations. In this course, we're going to use the `r pkg("correlation")` package which is part of the [easystats](https://easystats.github.io/easystats/) framework which tries to provide a unifying framework for analysis similar to how the tidyverse is a collection of packages for data wrangling and visualization.

Once again we'll be jumping between different datasets that best fit the analysis technique we are demonstrating so do take the time to familiarize yourself with the variables before you get started.

First, we'll use the `gapminder` dataset and look at the relationship between life expectancy and GDP per capita in the Americas. For good measure let's first calculate some descriptive statistics about our two variables before we progress to looking at the relationship between them. Rather than use `summarize()`, we'll use the `describe()` function from the `r pkg("psych")` package which very easily produces a range of descriptive stats:

```{r}
descriptives <- gapminder %>%
  filter(continent == "Americas") %>%
  select(lifeExp, gdpPercap) %>%
  describe()
```

We can then visualize the relationship:

```{r}
gapminder %>%
  filter(continent == "Americas") %>%
  ggplot(aes(x = lifeExp, y = gdpPercap)) +
  geom_point() +
  geom_smooth()
```

We can see quite a clear relationship between life expectancy and GDP, however, we can also see from the scatterplot and the skew stats that it's not a linear relationship, GDP is positively skewed, a fact that becomes even clearer if we visualize GDP with a histogram:

```{r}
gapminder %>%
  filter(continent == "Americas") %>%
  ggplot(aes(x = gdpPercap)) +
  geom_histogram()
```

There are different approaches to dealing with this skew, we'll show you two but other options are available and in addition to researching different theoretical approaches, it's worth looking at the help documentation for each analysis function to see all the options for more advanced analysis techniques. First, we could conduct a Spearman's correlation which is suitable for non-linear relationships as it transforms the data into ranks. 

The `correlation()` function from the `r pkg("easystats")` `r pkg("correlation")` package is a very nice function for performing correlations. Not only does it contain a huge number of options and types of correlation analysis (look at the help documentation), but it has been built to work well with the `r pkg("tidyverse")` which means that you can pipe into it and the output it produces is already tidy:

```{r}
spearman_results <- gapminder %>%
  filter(continent == "Americas") %>%
  correlation(select = "lifeExp", select2 = "gdpPercap", method = "spearman")

spearman_results
```

You may also wish to apply a log transformation to the GDP data. Not only will this fix the skew but it's potentially more appropriate to look at the relationship between relative differences in GDP rather than absolute. We can produce the log transformation by using the functions `mutate()` and `log()` / `log10()` or `log2()` depending on which transformation we wish to use.

```{r eval = FALSE}
gapminder <- gapminder %>%
  mutate(gdp_log = log10(gdpPercap))

# histogram of log transformed GDP
gapminder %>%
  filter(continent == "Americas") %>%
  ggplot(aes(x = gdp_log)) +
  geom_histogram()

# scatterplot of log GDP and life expectancy
gapminder %>%
  filter(continent == "Americas") %>%
  ggplot(aes(x = lifeExp, y = gdp_log)) +
  geom_point() +
  geom_smooth() 
```

```{r echo  = FALSE}
gapminder <- gapminder %>%
  mutate(gdp_log = log10(gdpPercap))

p1 <- gapminder %>%
  filter(continent == "Americas") %>%
  ggplot(aes(x = gdp_log)) +
  geom_histogram()

p2 <- gapminder %>%
  filter(continent == "Americas") %>%
  ggplot(aes(x = lifeExp, y = gdp_log)) +
  geom_point() +
  geom_smooth()

p1 + p2
```
It's not perfect but it's much better than it was and is much closer to a linear relationship (we'll assess whether this is really the case later) so we can use a Pearson correlation:

```{r}
pearson_results <- gapminder %>%
  filter(continent == "Americas") %>%
  correlation(select = "lifeExp", select2 = "gdpPercap", method = "pearson")

pearson_results
```

## Regression

### Linear regression

Constructing a regression model is a great example of the fact that when it comes to R, data cleaning and wrangling is by far the most difficult task you face. To construct a simple linear regression model use the function `lm()` (linear model):

* The formula is expressed as `outcome by predictor` so in this case, we're predicting life expectancy from GDP
* We use the notation `data = .` because we're piping in a filtered dataset to the function. If you wanted to run the regression on the full dataset, you'd remove the first two lines of pipes and just have `data = gapminder`
* `lm()` constructs the model whilst `summary()` summarizes the results.

```{r}
mod1 <- gapminder %>%
  filter(continent == "Americas") %>%
  lm(formula = lifeExp ~ gdp_log, data = .)

summary(mod1)
```

You can also pass the model summary to `tidy()` to pass the model stats to a tidy table:

```{r}
summary(mod1) %>%
  tidy()
```

It looks like we have significant model where GDP predicts life expectancy. This probably isn't a shock to anyone but let's check that our use of the linear model was really appropriate by assessing the model performance.

### Model performance

The `r pkg("performance")` package is also part of the `r pkg("easystats")` framework and provides a number of elegant and simple functions for assessing model performance. If you use any type of modelling in your research, we strongly recommended checking out the performance help documentation and [website](https://easystats.github.io/performance/index.html) for more information on what it can do.

You can test the assumptions of your model separately using functions like `check_heteroscedasticity()`, however, there's a single comprehensive function `check_model()` that provides an overview:

**If you get the error `Error: The RStudio 'Plots' window is too small to show this set of plots. Please make the window larger.` literally resize the bottom right window pane by dragging it with your cursor and then re-run the code.

```{r}
check_model(mod1)
```

It's not perfect, you can still see the influence of the skew, but it's good enough and we'll leave the in-depth theoretical debates about model performance to the statisticians!

### Multiple regression

Adding additional variables to your regression model is extremely simple. For this analysis, let's use the `polyps` dataset and as before, we'll remove the extreme outliers. This time, we're going to construct a model to see if we can predict the number of polyps after 12 months as a function of the baseline number of polyps (it's reasonable to assume that people with a baseline high number of polyps are going to have more polyps after treatment than those who started with a low number, even if the treatment is successful) and which treatment group they were in.

First, let's visualize this relationship.

```{r}
polyps_outliers <- polyps %>%
  filter(baseline < 100) # keep only those values less than 100

polyps_outliers %>%
  ggplot(aes(x = baseline, y = number12m, colour = treatment)) +
  geom_point() +
  geom_smooth(method ="lm")
```

Now, construct and summarize the model adding in two predictors:

```{r}
mod2 <-lm(number12m ~ baseline + treatment, data = polyps_outliers)
summary(mod2)

```

If you want to specify as interaction term, you can do so using the notation `predictor1:predictor2`:

```{r}
mod3 <-lm(number12m ~ baseline + treatment + baseline:treatment, data = polyps_outliers)
summary(mod3)
```

### Prediction

Once we have constructed our models, we can the model to predict new values. For example, let's create a table of data from new patients where we have their baseline number of polyps and which treatment condition they're due to receive. 

```{r}

new_patients <- tibble(
  participant_id = 1:10,
  baseline = c(20, 25, 30, 35, 40, 45, 50, 55, 60, 65),
  treatment = c("sulindac", "placebo", "sulindac", "placebo", "sulindac", "placebo", "sulindac", "placebo", "sulindac", "placebo")
)

```

We can then use `mutate()` and `predict()` to create a new variable `predicted_12m` that uses the model constructed in `mod2` to predict how many polyps these number patients would have at 12 months, taking into account their baseline and treatment group. Note that in order for the `predict()` function to work, you need to have data on all predictors entered in the original model (in this case baseline and treatment).

```{r}
new_patients <- new_patients %>%
  mutate(predicted_12m = predict(mod2, new_patients))

new_patients
```

### Logistic regression

Constructing a logistic regression model follows a very similar syntax but uses the function `glm()` (generalized linear model) rather than `lm()` as we've used previously. For this model, we'll use the `opt` dataset to see if we can predict whether a patient has chlamydia. First, let's create a reduced dataset of the variable we want to include in the model and check out our variables:

```{r}
chlamydia_dat <- opt %>%
  select(PID, Age, Black, White,Nat.Am, Asian, Hisp, Education, Chlamydia) 

summary(chlamydia_dat)
```

The summary output reveals an interesting quirk of this dataset. There's missing data in the `chlamydia` and `Hisp` variables but it's not represented as a missing value. Instead, it's been coded as blank but inexplicably not actually blank -  R thinks there is a type of data in the cell otherwise it would show up as `NA`. 

To deal with this we could either convert the blanks to NAs. This dataset is so badly coded that it took several attempts of trial-and-error to figure out what the blank was - it turns out it is three spaces and also "No" is stored as "No " with a space after it. Because we need our outcome to be a binary variable, we can use `droplevels` to get rid of the blank grouping (try removing the call to `droplevels` then run `summary()` to see how the data are represented).

```{r}
chlamydia_dat2 <- chlamydia_dat %>%
  mutate(Chlamydia = na_if(Chlamydia, "   "),
         Hisp = na_if(Hisp, "   ")) %>%
  droplevels()

summary(chlamydia_dat2)
```

We can then construct and summarize our model using `glm()` and `summary()` which tells us that the only significant predictor of having chlamydia from the measures we included is age:

```{r}
log_mod1 <- glm(formula = Chlamydia ~ Age + Black + White + Nat.Am + Asian + Hisp + Education, family = binomial, data = chlamydia_dat2)

summary(log_mod1)

```

## Further resources {#resources-analysis1}

There's so much more you can do in R when it comes to analysis techniques that we can't claim to have even begun to scratch the surface here. What's important is that from a coding perspective, you now have all the *programming* skill and knowledge you need to extend and expand your analytic workflows in R: from here on out, it's down to statistical theory and knowledge to take you further (which is why this course stops here, because Emily knows her limits).

If you have specific analytic techniques you'd like to use in R, please let us know ahead of the second office hour and we'll collate extra resources specific to your needs to help support your goals and Javan will also answer any questions you have about analytic techniques using the *All of Us* Researcher Workbench.

- [easystats](https://easystats.github.io/easystats/)
- [Learning statistics with R by Danielle Navarro](https://learningstatisticswithr.com/)
- [Learning statistical models through simulation in R](https://psyteachr.github.io/stat-models-v1/)
- [R for health data science](https://argoshare.is.ed.ac.uk/healthyr_book/)
- [Introduction to Modern Statistics](https://openintro-ims.netlify.app/index.html)
- [Statistical inference via data science](https://moderndive.com/)









